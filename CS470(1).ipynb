{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JtacuuRftec3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k30W4ZsrAB2m"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config values\n",
        "img_width = 1280\n",
        "img_height = 128\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "_Fozow5F4qX7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/mongol_bichig/dict_data.xlsx\"\n",
        "lyrics_dir0 = \"/content/drive/MyDrive/mongol_bichig/lyrics_data_0-4999.xlsx\"\n",
        "lyrics_dir1 = \"/content/drive/MyDrive/mongol_bichig/lyrics_data_5000-19999.xlsx\"\n",
        "lyrics_dir2 = \"/content/drive/MyDrive/mongol_bichig/lyrics_data_20000-29999.xlsx\"\n",
        "lyrics_dir3 = \"/content/drive/MyDrive/mongol_bichig/lyrics_data_30000-39999.xlsx\"\n",
        "lyrics_dir4 = \"/content/drive/MyDrive/mongol_bichig/lyrics_data_40000-49999.xlsx\"\n",
        "data = pd.read_excel(data_dir,header=None)\n",
        "lyrics0 = pd.read_excel(lyrics_dir0,header=None)\n",
        "lyrics1 = pd.read_excel(lyrics_dir1,header=None)\n",
        "lyrics2 = pd.read_excel(lyrics_dir2,header=None)\n",
        "lyrics3 = pd.read_excel(lyrics_dir3,header=None)\n",
        "lyrics4 = pd.read_excel(lyrics_dir4,header=None)\n",
        "\n",
        "train_size = 30720\n",
        "val_size = 10240\n",
        "test_size = 10240\n",
        "train_size = 800\n",
        "val_size = 32\n",
        "test_size = 32\n",
        "\n",
        "all_labels = np.concatenate([data[1].to_numpy(), lyrics0[1].to_numpy(), lyrics1[1].to_numpy(), lyrics2[1].to_numpy(), lyrics3[1].to_numpy(), lyrics4[1].to_numpy()])\n",
        "all_data = np.concatenate([data[0].to_numpy(), lyrics0[0].to_numpy(), lyrics1[0].to_numpy(), lyrics2[0].to_numpy(), lyrics3[0].to_numpy(), lyrics4[0].to_numpy()])\n",
        "max_length = max([len(label) for label in all_labels])\n",
        "\n",
        "np_label = all_labels[:train_size]\n",
        "np_data = all_data[:train_size]\n",
        "np_data = [f'/content/drive/MyDrive/mongol_bichig/images/dict-{int(i)}.png' for i in np_data]\n",
        "np_data = np.array(np_data)\n",
        "\n",
        "np_label_val = all_labels[train_size:(train_size+val_size)]\n",
        "np_data_val = all_data[train_size:(train_size+val_size)]\n",
        "np_data_val = [f'/content/drive/MyDrive/mongol_bichig/images/dict-{int(i)}.png' for i in np_data_val]\n",
        "np_data_val = np.array(np_data_val)\n",
        "\n",
        "np_label_test = all_labels[(train_size+val_size):(train_size+val_size+test_size)]\n",
        "np_data_test = all_data[(train_size+val_size):(train_size+val_size+test_size)]\n",
        "np_data_test = [f'/content/drive/MyDrive/mongol_bichig/images/dict-{int(i)}.png' for i in np_data_test]\n",
        "np_data_test = np.array(np_data_test)\n",
        "\n",
        "scripts = ['ᠰ', 'ᠡ', 'ᠭ', 'ᠥ', 'ᠳ', 'ᠯ', 'ᠵ', 'ᠬ', ' ', 'ᠠ', 'ᠤ', 'ᠴ', 'ᠪ', 'ᠷ', 'ᠦ', 'ᠢ', 'ᠮ', 'ᠨ', 'ᠱ', 'ᠣ', 'ᠲ', 'ᠶ', 'ᠺ', 'ᠹ', 'ᠧ', 'ᠫ', 'ᠩ', 'ᠼ', 'ᠸ', 'ᠻ', 'ᠾ', '᠍', 'ᠽ', '᠀', '᠁', '\\u180e', '\\u202f', '᠋', '᠌']\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(scripts.index(char))\n",
        "        except:\n",
        "          if (char == '/'):\n",
        "            dig_lst.append(len(scripts))\n",
        "\n",
        "    return dig_lst\n",
        "\n",
        "\n",
        "ocr_train_data = []\n",
        "ocr_train_label = []\n",
        "i = 0\n",
        "for label, data in zip(np_label, np_data):\n",
        "  try:\n",
        "    img = cv2.imread(data, 0)\n",
        "    img = cv2.transpose(img)\n",
        "    new_arr = cv2.resize(img, (img_width, img_height))\n",
        "    if ocr_train_data:\n",
        "      ocr_train_data = ocr_train_data + [new_arr]      \n",
        "    else:\n",
        "      ocr_train_data = [new_arr]\n",
        "    label = label.ljust(max_length, '/')\n",
        "    ocr_train_label.append(encode_to_labels(label))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "ocr_train_data_val = []\n",
        "ocr_train_label_val = []\n",
        "i = 0\n",
        "for label, data in zip(np_label_val, np_data_val):\n",
        "    try:\n",
        "      img = cv2.imread(data, 0)\n",
        "      new_arr = cv2.resize(img, (img_width, img_height))    \n",
        "      if ocr_train_data_val:\n",
        "        ocr_train_data_val = ocr_train_data_val + [new_arr]      \n",
        "      else:\n",
        "        ocr_train_data_val = [new_arr]\n",
        "      label = label.ljust(max_length, '/')\n",
        "      ocr_train_label_val.append(encode_to_labels(label))\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "ocr_train_data_test = []\n",
        "ocr_train_label_test = []\n",
        "i = 0\n",
        "for label, data in zip(np_label_test, np_data_test):\n",
        "    try:\n",
        "      img = cv2.imread(data, 0)\n",
        "      new_arr = cv2.resize(img, (img_width, img_height))    \n",
        "      if ocr_train_data_test:\n",
        "        ocr_train_data_test = ocr_train_data_test + [new_arr]      \n",
        "      else:\n",
        "        ocr_train_data_test = [new_arr]\n",
        "      label = label.ljust(max_length, '/')\n",
        "      ocr_train_label_test.append(encode_to_labels(label))\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "SCWofQnA1A6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the data and labels into numpy array \n",
        "\n",
        "ocr_train_label = np.array([np.array(xi) for xi in ocr_train_label])\n",
        "ocr_train_label = np.asarray(ocr_train_label).astype(float)\n",
        "ocr_train_data = np.array(ocr_train_data) / 255\n",
        "\n",
        "ocr_train_label_val = np.array([np.array(xi) for xi in ocr_train_label_val])\n",
        "ocr_train_label_val = np.asarray(ocr_train_label_val).astype(float)\n",
        "ocr_train_data_val = np.array(ocr_train_data_val) / 255\n",
        "\n",
        "ocr_train_label_test = np.array([np.array(xi) for xi in ocr_train_label_test])\n",
        "ocr_train_label_test = np.asarray(ocr_train_label_test).astype(float)\n",
        "ocr_train_data_test = np.array(ocr_train_data_test) / 255"
      ],
      "metadata": {
        "id": "cdv-LDb118JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the data\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
        "img = (ocr_train_data[0, :, :] * 255).astype(np.uint8)\n",
        "img = img.T\n",
        "ax.imshow(img, cmap=\"gray\")\n",
        "ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sEH_W5r7h2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Accuracy Metric"
      ],
      "metadata": {
        "id": "LxecSHmvZo6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6QZeZmc4ZbPU"
      },
      "outputs": [],
      "source": [
        "# This is the custom accuracy metric that I created that returns character\n",
        "# accuracy instead of word accuracy. \n",
        "\n",
        "# update_state() does the calculations and updates the state variables\n",
        "# result() returns the accuracy score as a fraction/decimal\n",
        "# reset_state() resets the state variables after each epoch.\n",
        "class SequenceAccuracy(keras.metrics.Metric):\n",
        "  def __init__(self, name='sequence_accuracy', **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.total = self.add_weight(name='total', initializer='zeros')\n",
        "    self.count = self.add_weight(name='count', initializer='zeros')\n",
        "              \n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    def sparse2dense(tensor, shape):\n",
        "      tensor = tf.sparse.reset_shape(tensor, shape)\n",
        "      tensor = tf.sparse.to_dense(tensor, default_value=len(scripts))\n",
        "      tensor = tf.cast(tensor, tf.float32)\n",
        "      return tensor\n",
        "\n",
        "    def dense2sparse2dense(tensor, shape):\n",
        "      tensor = tf.sparse.from_dense(tensor=tensor)\n",
        "      tensor = tf.sparse.reset_shape(tensor, shape)\n",
        "      tensor = tf.sparse.to_dense(tensor, default_value=len(scripts))\n",
        "      tensor = tf.cast(tensor, tf.float32)\n",
        "      return tensor\n",
        "\n",
        "    y_true_shape = tf.shape(y_true)\n",
        "    y_pred_shape = tf.shape(y_pred)\n",
        "    max_width = tf.math.maximum(y_true_shape[1], y_pred_shape[2])\n",
        "    logit_length = tf.fill([batch_size], y_pred_shape[2])\n",
        "    decoded, _ = tf.nn.ctc_greedy_decoder(\n",
        "        inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n",
        "        sequence_length=logit_length)\n",
        "    y_true = dense2sparse2dense(y_true, [batch_size, max_width])\n",
        "    y_pred = sparse2dense(decoded[0], [batch_size, max_width])\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(batch_size):\n",
        "      for j in range(max_width):\n",
        "        if (not (y_true[i][j] == len(scripts) and y_pred[i][j] == len(scripts))):\n",
        "          total += 1\n",
        "          if (y_pred[i][j] == y_true[i][j]):\n",
        "            correct += 1\n",
        "    total = tf.cast(total, tf.float32)\n",
        "    correct = tf.cast(correct, tf.float32)\n",
        "    self.total.assign_add(total)\n",
        "    self.count.assign_add(correct)\n",
        "\n",
        "  def result(self):\n",
        "    return self.count / self.total\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.count.assign(0)\n",
        "    self.total.assign(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CTC Loss Function"
      ],
      "metadata": {
        "id": "oeLEmd9bZvzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # just return the computed predictions\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "2qmuh5pFYzoB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "3KyCwjl775hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtwA8tEYZb48"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    # Inputs to the model\n",
        "    input_img = layers.Input(\n",
        "        shape=(img_height, img_width, 1), name=\"image\",\n",
        "    )\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "\n",
        "    # First conv block\n",
        "    x = layers.Conv2D(\n",
        "        64,\n",
        "        (2, 2),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv1\",\n",
        "    )(input_img)\n",
        "    x = layers.MaxPooling2D((2, 2), strides=2, name=\"pool1\")(x)\n",
        "\n",
        "    # Second conv block\n",
        "    x = layers.Conv2D(\n",
        "        64,\n",
        "        (2, 2),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv2\",\n",
        "    )(x)\n",
        "    x = layers.MaxPooling2D((2, 2), strides=2, padding=\"same\", name=\"pool2\")(x)\n",
        "\n",
        "    new_shape = ((img_width // 8), (img_height // 2) * 64)\n",
        "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # RNNs\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(\n",
        "        len(scripts) + 1, activation=\"softmax\", name=\"dense2\"\n",
        "    )(x)\n",
        "\n",
        "    # Add CTC layer for calculating CTC loss at each step\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.models.Model(\n",
        "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam()\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, metrics=[SequenceAccuracy()])\n",
        "    return model\n",
        "\n",
        "# Get the model\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "mJd4X6A18NbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    # Inputs to the model\n",
        "    input_img = layers.Input(\n",
        "        shape=(img_height, img_width, 1), name=\"image\",\n",
        "    )\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "\n",
        "    x = layers.Conv2D(64, (3,3), activation = 'relu', padding='same', name=\"Conv1\")(input_img)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), name=\"pool1\")(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, (3,3), activation = 'relu', padding='same', name=\"Conv2\")(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), name=\"pool2\")(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, (3,3), activation = 'relu', padding='same', name=\"Conv3\")(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, (3,3), activation = 'relu', padding='same', name=\"Conv4\")(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 1), name=\"pool3\")(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, (3,3), activation = 'relu', padding='same', name=\"Conv5\")(x)\n",
        "    x = layers.BatchNormalization(name=\"norm1\")(x)\n",
        "    \n",
        "    new_shape = ((img_width // 4), (img_height // 1) * 64) \n",
        "    x = layers.Reshape(target_shape=new_shape)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2), name=\"bidirec1\")(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2), name=\"bidirec2\")(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(\n",
        "        len(scripts) + 1, activation=\"softmax\", name=\"dense2\"\n",
        "    )(x)\n",
        "\n",
        "    # Add CTC layer for calculating CTC loss at each step\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.models.Model(\n",
        "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v2\"\n",
        "    )\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam()\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, metrics=[SequenceAccuracy()])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7ztxbcCohCDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3"
      ],
      "metadata": {
        "id": "JPYTQWew8R1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "def build_model():\n",
        "    inputs = layers.Input(\n",
        "        shape=(img_height, img_width, 1), name=\"image\",\n",
        "    )\n",
        "\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
        "\n",
        "    conv_1 = layers.Conv2D(32, (3,3), activation = \"relu\", padding='same', name=\"conv1\")(inputs)\n",
        "    pool_1 = layers.MaxPool2D(pool_size=(2, 1), name=\"pool1\")(conv_1)\n",
        "    \n",
        "    conv_2 = layers.Conv2D(64, (3,3), activation = \"relu\", padding='same', name=\"conv2\")(pool_1)\n",
        "    pool_2 = layers.MaxPool2D(pool_size=(3, 2), name=\"pool2\")(conv_2)\n",
        "\n",
        "    conv_3 = layers.Conv2D(128, (3,3), activation = \"relu\", padding='same', name=\"conv3\")(pool_2)\n",
        "    pool_3 = layers.MaxPool2D(pool_size=(3, 2), name=\"pool3\")(conv_3)\n",
        "   \n",
        "    drop_1 = layers.Dropout(0.2, name=\"drop1\")(pool_3)\n",
        "    conv_4 = layers.Conv2D(128, (3,3), activation = \"relu\", padding='same', name=\"conv4\")(drop_1)\n",
        "\n",
        "    pool_4 = layers.MaxPool2D(pool_size=(3, 2), name=\"pool4\")(conv_4)\n",
        "\n",
        "    conv_5 = layers.Conv2D(256, (3,3), activation = \"relu\", padding='same', name=\"conv5\")(pool_4)\n",
        "    \n",
        "    # Batch normalization layer\n",
        "    batch_norm_5 = layers.BatchNormalization(name=\"batch1\")(conv_5)\n",
        "    conv_6 = layers.Conv2D(256, (3,3), activation = \"relu\", padding='same', name=\"conv6\")(batch_norm_5)\n",
        "    batch_norm_6 = layers.BatchNormalization(name=\"batch2\")(conv_6)\n",
        "    \n",
        "    conv_7 = layers.Conv2D(64, (2,2), activation = \"relu\", name=\"conv7\")(batch_norm_6)\n",
        "    \n",
        "    squeezed = layers.Lambda(lambda x: K.squeeze(x, 1), name=\"sqeeze\")(conv_7)\n",
        "    \n",
        "    # bidirectional LSTM layers \n",
        "    blstm_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2), name=\"lstm1\")(squeezed)\n",
        "    blstm_2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2), name=\"lstm2\")(blstm_1)\n",
        "\n",
        "    softmax_output = layers.Dense(len(scripts) + 1, activation = 'softmax', name=\"dense\")(blstm_2)\n",
        "\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, softmax_output)\n",
        "\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # create and return model\n",
        "    model = keras.models.Model(inputs=[inputs, labels], outputs=output, name=\"ocr_model_v3\")\n",
        "    model.compile(optimizer = optimizer, metrics=[SequenceAccuracy()])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6P7vA3836qgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrtjI96IZd9G"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [ocr_train_data, ocr_train_label],\n",
        "    ocr_train_label,\n",
        "    validation_data=[ocr_train_data_val, ocr_train_label_val],\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores = model.evaluate([ocr_train_data_test, ocr_train_label_test], ocr_train_label_test, verbose=1)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "metadata": {
        "id": "kR0KJhU6Y3gz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS470",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}